{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Style Transfer\n",
    "\n",
    "In this project we trained a custom Neural Style Transfer (NST) model that can take a realisic content image and apply a style (painting).\n",
    "\n",
    "Dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optimization\n",
    "import numpy as np\n",
    "\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll crop the original images (content and style) to be at the same dimenstions. We'll define a crop function that crops 512x512 pixels from the center of each image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_it(img):\n",
    "    input_path = Path(img)\n",
    "    image = Image.open(img)\n",
    "    width, height = image.size\n",
    "    left = (width-512)/2\n",
    "    right = left+512\n",
    "    top = (height-512)/2\n",
    "    bottom = top+512\n",
    "    cropped_img = image.crop((left, top, right, bottom))\n",
    "    modified_path = input_path.with_name(f\"{input_path.stem}_crop{input_path.suffix}\")\n",
    "    cropped_img.save(modified_path)\n",
    "    return cropped_img.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply it on each image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Image.show of <PIL.Image.Image image mode=RGB size=512x512 at 0x1B7056462E0>>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crop_it('Path/to/style.jpg')\n",
    "crop_it('Path/to/content.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions\n",
    "### Content loss function\n",
    "\n",
    "We'll implement a function to calculate the content loss which is the squared error loss between the two feature vectors of the content image and the target image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_loss(target_vec, content_vec):\n",
    "  return torch.mean((target_vec-content_vec)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style loss function\n",
    "\n",
    "We'll implement a function to calculate the style loss by using Gram matrix. The total loss is the sum of every mean-squared distance (between two gram matrices of the style and the target images) for every layer times the weighted factor (the influence factor) of every layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(input, c, h, w):\n",
    "  #c-channels; h-height; w-width \n",
    "  input = input.view(c, h*w) \n",
    "  #matrix multiplication on its own transposed form\n",
    "  G = torch.mm(input, input.t())\n",
    "  return G\n",
    "  \n",
    "def get_style_loss(target, style):\n",
    "  _, c, h, w = target.size()\n",
    "  G = gram_matrix(target, c, h, w) #gram matrix for the target image\n",
    "  S = gram_matrix(style, c, h, w) #gram matrix for the style image\n",
    "  return torch.mean((G-S)**2)/(c*h*w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model (loading a pretrained VGG19 and modifying it)\n",
    "We will use only 5 layers from the model (conv layers) just for feature extraction. We remove other layers used for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "class VGG(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(VGG, self).__init__()\n",
    "    self.select_features = ['0', '5', '10', '19', '28'] #conv layers\n",
    "    self.vgg = models.vgg19(pretrained=True).features\n",
    "  \n",
    "  def forward(self, output):\n",
    "    features = []\n",
    "    for name, layer in self.vgg._modules.items():\n",
    "      output = layer(output)\n",
    "      if name in self.select_features:\n",
    "        features.append(output)\n",
    "    return features\n",
    "\n",
    "#load the model\n",
    "vgg = VGG().to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load image function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#preprocessing of the images\n",
    "loader = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),  # Resize image\n",
    "    transforms.ToTensor(),          # Convert image to tensor\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "def random_noise(device='cuda'):\n",
    " height, width = content_img.shape[2:]\n",
    " output_img = np.random.randn(3, height, width) # (3, H, W)\n",
    " noise_tensor = torch.tensor(output_img, dtype=torch.float32).unsqueeze(0) # (1, 3, H, W)\n",
    " return noise_tensor\n",
    "\n",
    "# Let's define a function to load images (style, content)\n",
    "def load_img(path):\n",
    "  img = Image.open(path)\n",
    "  img = loader(img).unsqueeze(0)\n",
    "  return img.to(device)\n",
    "\n",
    "content_img = load_img('Path/to/content_crop.jpg')\n",
    "style_img = load_img('Path/to/style_crop.jpg')\n",
    "#we can start by copying the content image as a starting point\n",
    "target_img = random_noise()\n",
    "#target_img = torch.from_numpy(target_img)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "target_img = target_img.to(device)\n",
    "img_size = 512 if torch.cuda.is_available() else 128\n",
    "target_img = torch.randn_like(content_img, device=device, requires_grad=True)\n",
    "\n",
    "#initial model\n",
    "model = models.vgg19(pretrained=True).features\n",
    "#the optimizer: We use Adam since it's generally more adequate\n",
    "optimizer = optimization.Adam([target_img], lr=0.001)\n",
    "\n",
    "alpha = 50 #content wight\n",
    "beta = 50 #style weight\n",
    "\n",
    "#define the load_img function first\n",
    "content_img = load_img('Path/to/content_crop.jpg')\n",
    "style_img = load_img('Path/to/style_crop.jpg')\n",
    "#we can start from a random noise generated image or \n",
    "#just copy the content image as a starting point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(target_img.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "print(target_img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/10000 [00:00<12:48, 13.00it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Loss = 1947876.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 501/10000 [02:58<3:36:00,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500: Loss = 20966.8379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1008/10000 [05:57<1:04:11,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000: Loss = 13274.2188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 1501/10000 [08:21<2:36:36,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1500: Loss = 8583.6123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2001/10000 [10:46<2:28:39,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2000: Loss = 4453.6289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2501/10000 [13:11<2:18:58,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2500: Loss = 2856.5024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3010/10000 [15:36<27:40,  4.21it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3000: Loss = 2346.8604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 3501/10000 [17:59<1:58:34,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3500: Loss = 2053.1338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4001/10000 [20:23<1:49:29,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4000: Loss = 1832.6255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 4501/10000 [22:47<1:40:12,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4500: Loss = 1651.9519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5001/10000 [25:10<1:31:06,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5000: Loss = 1498.0325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 5501/10000 [27:35<1:23:03,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5500: Loss = 1361.8513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6001/10000 [30:01<1:14:06,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6000: Loss = 1240.9624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 6510/10000 [2:18:33<16:14,  3.58it/s]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6500: Loss = 1135.9574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7010/10000 [2:21:26<14:08,  3.52it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7000: Loss = 1049.2941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 7309/10000 [2:23:10<52:42,  1.18s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import save_image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define denormalization (assuming you used these values for normalization)\n",
    "denormalization = transforms.Normalize((-2.12, -2.04, -1.80), (4.37, 4.46, 4.44))\n",
    "\n",
    "# Function to save the target image\n",
    "def save(target, i):\n",
    "    img = target.clone().detach().squeeze()  # Remove gradients\n",
    "    img = denormalization(img).clamp(0, 1)   # Apply denormalization and clamp values\n",
    "    save_image(img, f'result_{i}.png')\n",
    "\n",
    "# Set device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the target image (learnable)\n",
    "target_img = torch.randn_like(content_img, device=device, requires_grad=True)\n",
    "\n",
    "# Optimizer (Adam is smoother, LBFGS is used in the original NST paper)\n",
    "optimizer = optim.Adam([target_img], lr=0.01)\n",
    "\n",
    "# Compute content and style features once (no need to compute them every step)\n",
    "content_feature = [f.detach() for f in vgg(content_img)]\n",
    "style_feature = [f.detach() for f in vgg(style_img)]\n",
    "#content_feature = vgg(content_img).detach()  # No need to track gradients\n",
    "#style_feature = vgg(style_img).detach()\n",
    "\n",
    "# Training loop\n",
    "steps = 10000\n",
    "for step in tqdm(range(steps)):\n",
    "    # Compute target features\n",
    "    target_feature = vgg(target_img)\n",
    "\n",
    "    # Compute losses\n",
    "    #content_loss = get_content_loss(target_feature, content_feature)\n",
    "    content_loss = sum(get_content_loss(t, c) for t, c in zip(target_feature, content_feature))\n",
    "    style_loss = sum(get_style_loss(t, c) for t, c in zip(target_feature, style_feature))\n",
    "\n",
    "\n",
    "    #style_loss = get_style_loss(target_feature, style_feature)\n",
    "\n",
    "    # Calculate total loss\n",
    "    total_loss = alpha * content_loss + beta * style_loss\n",
    "\n",
    "    # Zero out previous gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Compute gradients\n",
    "    total_loss.backward()\n",
    "\n",
    "    # Update the target image\n",
    "    optimizer.step()\n",
    "\n",
    "    # Save progress every 500 steps\n",
    "    if step % 500 == 0:\n",
    "        save(target_img, step)\n",
    "        print(f\"Step {step}: Loss = {total_loss.item():.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
